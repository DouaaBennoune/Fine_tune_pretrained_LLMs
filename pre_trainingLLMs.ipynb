{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c7175e",
   "metadata": {},
   "source": [
    "# Pre-training LLMs with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "!pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch=2.1.0+cu118\n",
    "# # - Update a specific package\n",
    "!pip install pmdarima -U\n",
    "# # - Update a package to specific version\n",
    "!pip install --upgrade pmdarima==2.0.2\n",
    "# # Note: If your environment doesn't support \"!pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.0 \n",
    "!pip install -U git+https://github.com/huggingface/transformers\n",
    "!pip install --user datasets # 2.15.0\n",
    "!pip install --user portalocker>=2.0.0\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U accelerate\n",
    "!pip install --user torch==2.3.0\n",
    "!pip install -U torchvision\n",
    "!pip install --user protobuf==3.20\n",
    "!pip install --user dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404057c",
   "metadata": {},
   "source": [
    "- !pip install transformers==4.40.0\n",
    "\n",
    "Installs version 4.40.0 of the transformers library. This library by Hugging Face provides pre-trained models and tools for natural language processing (NLP) and machine learning.\n",
    "______________________________________________________________\n",
    "- !pip install -U git+https://github.com/huggingface/transformers\n",
    "\n",
    "Installs the latest version of the transformers library directly from Hugging Face's GitHub repository (-U ensures upgrading if a version is already installed).\n",
    "______________________________________________________________\n",
    "- !pip install --user datasets # 2.15.0\n",
    "\n",
    "Installs the datasets library (version 2.15.0 based on the comment). This library, also by Hugging Face, is used for loading and manipulating datasets.\n",
    "______________________________________________________________\n",
    "- !pip install --user portalocker>=2.0.0\n",
    "\n",
    "Installs version 2.0.0 or higher of the portalocker library, which helps in file locking for cross-platform applications.\n",
    "______________________________________________________________\n",
    "- !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "\n",
    "Silently (-q for quiet mode) installs the latest version of the accelerate library from its GitHub repository. This library optimizes and scales machine learning workflows.\n",
    "______________________________________________________________\n",
    "- !pip install -q -U accelerate\n",
    "\n",
    "Updates the accelerate library to the latest version. If it's already installed, it ensures it's up-to-date.\n",
    "______________________________________________________________\n",
    "- !pip install --user torch==2.3.0\n",
    "\n",
    "Installs version 2.3.0 of PyTorch, a popular machine learning library for deep learning.\n",
    "______________________________________________________________\n",
    "- !pip install -U torchvision\n",
    "\n",
    "Updates the torchvision library, which provides tools and datasets for computer vision tasks.\n",
    "______________________________________________________________\n",
    "- !pip install --user protobuf==3.20\n",
    "\n",
    "Installs version 3.20 of the protobuf library, which is used for serializing structured data and is often required by machine learning frameworks.\n",
    "______________________________________________________________\n",
    "- !pip install --user dataset\n",
    "\n",
    "Installs the dataset library. This is different from Hugging Face's datasets library and is used to work with databases in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2d464",
   "metadata": {},
   "source": [
    "# TOKENIZERS_PARALLELISM to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef5133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable TOKENIZERS_PARALLELISM to 'false'\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb11f65",
   "metadata": {},
   "source": [
    "# Pretraining and self-supervised fine-tuning \n",
    "lets load the pretrained model and make a inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce36ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"facebook/opt-350\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\n",
    "print(pipe(\"this movie was really\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad267917",
   "metadata": {},
   "source": [
    "# Self-supervised training of a BERT model\n",
    "Prepare the train dataset\n",
    "Train a Tokenizer\n",
    "Preprocess the dataset\n",
    "Pre-train BERT using an MLM task\n",
    "Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset=load_dataset(\"wikitext\",\"wikitext-2-raw-v1\")\n",
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "dataset[\"test\"] = dataset[\"test\"].select([i for i in range(200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dded395",
   "metadata": {},
   "source": [
    "Why Create Text Files?\n",
    "\n",
    "These files serve as input to the TextDataset object, which is commonly used in NLP tasks like pretraining language models. Here's the workflow:\n",
    "\n",
    "Extract Raw Data: Text is extracted from datasets and saved to text files.\n",
    "\n",
    "Preprocessing: Text files are loaded into objects like TextDataset, where tokenization and formatting happen.\n",
    "\n",
    "Training Models: Preprocessed datasets are then fed into models for training using objectives like Masked Language Modeling (MLM).\n",
    "\n",
    "## Step 1: Specify File Paths\n",
    "These lines define the paths where the training and test datasets will be saved as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_train = \"wikitext_dataset_train.txt\"\n",
    "output_file_test = \"wikitext_dataset_test.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f997b3",
   "metadata": {},
   "source": [
    "## Step 2: Save Training Dataset\n",
    "- Open File: The training file (wikitext_dataset_train.txt) is opened in write mode (\"w\"), with UTF-8 encoding to handle special characters.\n",
    "\n",
    "- Iterate Through Examples: Loop through each example in the training set (dataset[\"train\"]).\n",
    "\n",
    "- Write to File: For every example, write the text field from the dataset to the file, adding a newline (\"\\n\") after each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in dataset[\"train\"]:\n",
    "        f.write(example[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313cddc",
   "metadata": {},
   "source": [
    "## Step 3: Save Test Dataset\n",
    "This is almost identical to the training dataset step, except it processes the test set (dataset[\"test\"]) and writes to a separate file (wikitext_dataset_test.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_test, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in dataset[\"test\"]:\n",
    "        f.write(example[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943a445",
   "metadata": {},
   "source": [
    "You need to define a tokenizer to be used for tokenizing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer from existing one to re-use special tokens\n",
    "bert_tokenizer=BertTokenizerfast.from_pretrained(\"bert-base-uncased\")\n",
    "model_name='bert-base-uncased'\n",
    "model=AutoModelForCausalLm.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name,is_decoder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ff1a9",
   "metadata": {},
   "source": [
    "# Training a Tokenizer(Optional)\n",
    "This is specially helpful when using transformers for specific areas such as medicine where tokens are somehow different than the general tokens that tokenizers are created based on. (You can skip this step if you do not want to train the tokenizer on your specific data):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37226c",
   "metadata": {},
   "source": [
    "## Part 1: Create a Python Generator to Dynamically Load the Data(look the code below )\n",
    "1-def batch_iterator(batch_size=10000):\n",
    "Defines a function named batch_iterator that serves as a Python generator.\n",
    "Accepts a parameter batch_size, which specifies the number of examples to be processed in each batch. The default value is set to 10000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a python generator to dynamically load the data\n",
    "#1\n",
    "def batch_iterator(batch_size=10000):\n",
    "    #2\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        #3\n",
    "        yield dataset['train'][i : i + batch_size][\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c181d9",
   "metadata": {},
   "source": [
    "2- for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "Uses a for loop to iterate through the dataset in increments of batch_size.\n",
    "\n",
    "range(0, len(dataset), batch_size) generates indices from 0 to the length of the dataset, stepping by batch_size. For example, if the dataset has 50,000 examples and batch_size is 10,000, the loop iterates over indices [0, 10000, 20000, 30000, 40000].\n",
    "\n",
    "tqdm adds a progress bar to visualize the loop's progress.\n",
    "\n",
    "3- yield dataset['train'][i : i + batch_size][\"text\"]\n",
    "\n",
    "The yield keyword makes the function a generator, producing one batch of data at a time rather than loading all the data into memory at once.\n",
    "\n",
    "Fetches a slice of the training dataset (dataset['train'][i : i + batch_size]), containing batch_size examples from the current position i.\n",
    "\n",
    "Extracts only the \"text\" field from the sliced examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5a53d",
   "metadata": {},
   "source": [
    "# Part 2: Create a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a7280",
   "metadata": {},
   "source": [
    "# Part 3: Train the Tokenizer Using the Dataset\n",
    "train_new_from_iterator\n",
    "\n",
    "Trains a new tokenizer by updating the vocabulary from the dataset.\n",
    "\n",
    "Uses an iterator (in this case, the generator function batch_iterator) to supply text data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63a35a",
   "metadata": {},
   "source": [
    "text_iterator=batch_iterator()\n",
    "\n",
    "Passes the generator batch_iterator() to the tokenizer training process. This ensures that text data is loaded dynamically in batches.\n",
    "\n",
    "vocab_size=30522\n",
    "\n",
    "Specifies the size of the vocabulary to build during training. In this case, the size matches the original BERT tokenizer vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc39ccf",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "\n",
    "In this step, we define the configuration of the BERT model and create the model:\n",
    "#### Define the BERT Configuration\n",
    "Here, we define the configuration settings for a BERT model using `BertConfig`. This includes setting various parameters related to the model's architecture:\n",
    "- **vocab_size=30522**: Specifies the size of the vocabulary. This number should match the vocabulary size used by the tokenizer.\n",
    "- **hidden_size=768**: Sets the size of the hidden layers.\n",
    "- **num_hidden_layers=12**: Determines the number of hidden layers in the transformer model.\n",
    "- **num_attention_heads=12**: Sets the number of attention heads in each attention layer.\n",
    "- **intermediate_size=3072**: Specifies the size of the \"intermediate\" (i.e., feed-forward) layer within the transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e616c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=len(bert_tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n",
    "    hidden_size=768,  # Set the hidden size\n",
    "    num_hidden_layers=12,  # Set the number of layers\n",
    "    num_attention_heads=12,  # Set the number of attention heads\n",
    "    intermediate_size=3072,  # Set the intermediate size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154d21b",
   "metadata": {},
   "source": [
    "now we  Create the BERT model for pre-training:\n",
    "and check model configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc80887",
   "metadata": {},
   "source": [
    "### Tokenize Dataset Dynamically\n",
    "\n",
    "In this section, we dynamically tokenize the dataset \n",
    "This approach provides greater flexibility and integrates well with modern NLP workflows.\n",
    "\n",
    "#### **Tokenization Function**\n",
    "The `tokenize_function` is used to preprocess the text data by tokenizing and formatting it for model training.\n",
    "\n",
    "#### What This Code Does as a Whole\n",
    "Defines a function to tokenize text examples using the BERT tokenizer.\n",
    "\n",
    "Applies the tokenizer to the training and testing datasets, dynamically processing data in batches.\n",
    "\n",
    "Removes the original raw text to save space and focuses on tokenized inputs.\n",
    "\n",
    "Splits the processed dataset into train and test subsets, preparing it for the next stages of model training and evaluation.\n",
    "\n",
    "This process ensures that your dataset is formatted and ready for input into a model like BERT. Would you like help with the next steps, such as feeding this data into a model for training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db149e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset dynamically\n",
    "def tokenize_function(examples):\n",
    "    return bert_tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Tokenize train and test datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Print tokenized dataset sample\n",
    "print(tokenized_datasets[\"train\"][0])\n",
    "\n",
    "# Split into training and test sets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "#examining  one sample the token indexes  are shown here with the block size.\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd148d5",
   "metadata": {},
   "source": [
    "bert_tokenizer(examples[\"text\"], ...):\n",
    "\n",
    "Applies the bert_tokenizer to the \"text\" field of the input examples.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "truncation=True: Ensures that text longer than max_length (512 tokens) is truncated.\n",
    "\n",
    "padding=\"max_length\": Pads shorter sequences to the maximum length (max_length=512) for consistent input size.\n",
    "\n",
    "max_length=512: Limits each input sequence to a maximum of 512 tokens.\n",
    "\n",
    "tokenize_function: This line applies the tokenize_function to the dataset.\n",
    "\n",
    "dataset.map(...):\n",
    "\n",
    "Maps (applies) the tokenize_function to each example in the dataset.\n",
    "\n",
    "The batched=True argument means examples are processed in batches rather than one at a time, which is more efficient.\n",
    "\n",
    "remove_columns=[\"text\"]:\n",
    "\n",
    "Removes the original \"text\" field from the dataset after tokenization (as it's no longer needed).\n",
    "\n",
    "Output:\n",
    "\n",
    "Produces a new dataset (tokenized_datasets) where each example contains tokenized inputs instead of raw text.\n",
    "\n",
    "tokenized_datasets[\"train\"][0]:\n",
    "\n",
    "Accesses the first example from the training dataset.\n",
    "\n",
    "It will show tokenized data (numerical tokens and possibly attention masks) instead of raw text.\n",
    "\n",
    "splits the tokenized dataset into training and testing subsets.\n",
    "\n",
    "train_dataset:\n",
    "\n",
    "Stores the tokenized training data, which is used to train the model.\n",
    "\n",
    "test_dataset:\n",
    "\n",
    "Stores the tokenized test data, which is used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54d4d4",
   "metadata": {},
   "source": [
    "Then, we prepare data for the MLM task (masking random tokens):\n",
    "### Define the Data Collator for Language Modeling\n",
    "This line of code sets up a `DataCollatorForLanguageModeling` from the Hugging Face Transformers library. A data collator is used during training to dynamically create batches of data. For language modeling, particularly for models like BERT that use masked language modeling (MLM), this collator prepares training batches by automatically masking tokens according to a specified probability. Here are the details of the parameters used:\n",
    "\n",
    "- **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used with the data collator. The `bert_tokenizer` is responsible for tokenizing the text and converting it to the format expected by the model.\n",
    "- **mlm=True**: Indicates that the data collator should mask tokens for masked language modeling training. This parameter being set to `True` configures the collator to randomly mask some of the tokens in the input data, which the model will then attempt to predict.\n",
    "- **mlm_probability=0.15**: Sets the probability with which tokens will be masked. A probability of 0.15 means that, on average, 15% of the tokens in any sequence will be replaced with a mask token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "# check how collator transforms a sample input data record\n",
    "data_collator([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fca14",
   "metadata": {},
   "source": [
    "Now, we train the BERT Model using the Trainer module. (For a complete list of training arguments, check [here](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.TrainingArguments)):\n",
    "This section configures the training process by specifying various parameters that control how the model is trained, evaluated, and saved:\n",
    "\n",
    "- **output_dir=\"./trained_model\"**: Specifies the directory where the trained model and other output files will be saved.\n",
    "- **overwrite_output_dir=True**: If set to `True`, this will overwrite the contents of the output directory if it already exists. This is useful when running experiments multiple times.\n",
    "- **do_eval=True**: Enables evaluation of the model. If `True`, the model will be evaluated at the specified intervals.\n",
    "- **evaluation_strategy=\"epoch\"**: Defines when the model should be evaluated. Setting this to \"epoch\" means the model will be evaluated at the end of each epoch.\n",
    "- **learning_rate=5e-5**: Sets the learning rate for training the model. This is a typical learning rate for fine-tuning BERT-like models.\n",
    "- **num_train_epochs=10**: Specifies the number of training epochs. Each epoch involves a full pass over the training data.\n",
    "- **per_device_train_batch_size=2**: Sets the batch size for training on each device. This should be set based on the memory capacity of your hardware.\n",
    "- **save_total_limit=2**: Limits the total number of model checkpoints to be saved. Only the most recent two checkpoints will be kept.\n",
    "- **logging_steps=20**: Determines how often to log training information, which can help monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    overwrite_output_dir=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=2,  # Set the batch size for training\n",
    "    save_total_limit=2,  # Limit the total number of saved checkpoints\n",
    "    logging_steps = 20\n",
    "    \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efadcd",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "Let's check the performance of the trained model. Perplexity is commonly used to compare different language models or different configurations of the same model.\n",
    "After training, perplexity can be calculated on a held-out evaluation dataset to assess the model's performance. The perplexity is calculated by feeding the evaluation dataset through the model and comparing the predicted probabilities of the target tokens with the actual token values that are masked.\n",
    "\n",
    "A lower perplexity score indicates that the model has a better understanding of the language and is more effective at predicting the masked tokens. It suggests that the model has learned useful representations and can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d220d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44510f6",
   "metadata": {},
   "source": [
    "## Loading the saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58045d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'\n",
    "model.resize_token_embeddings(30522)\n",
    "model.load_state_dict(torch.load('bert-scratch-model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea79de",
   "metadata": {},
   "source": [
    "The simplest way to try out the model for inference is to use it in a pipeline(). Instantiate a pipeline for fill-mask with your model, and pass your text to it. If you like, you can use the top_k parameter to specify how many predictions to return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7226f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text with a masked token\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Create a pipeline for the \"fill-mask\" task\n",
    "mask_filler = pipeline(\"fill-mask\", model=model,tokenizer=bert_tokenizer)\n",
    "\n",
    "# Generate predictions by filling the mask in the input text\n",
    "results = mask_filler(text) #top_k parameter can be set \n",
    "\n",
    "# Print the predicted sequences\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f2bc5",
   "metadata": {},
   "source": [
    "You can see that [MASK] is replaced by the most frequent token. This weak performance can be due to insufficient training, lack of training data, model architecture, or not tuning hyperparameters. Let's try a pretrained model from Hugging Face:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d7c03",
   "metadata": {},
   "source": [
    "## Inferencing a pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained BERT model and tokenizer\n",
    "pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the input text with a masked token\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Create the pipeline\n",
    "mask_filler = pipeline(task='fill-mask', model=pretrained_model,tokenizer=pretrained_tokenizer)\n",
    "\n",
    "# Perform inference using the pipeline\n",
    "results = mask_filler(text)\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0675d1b",
   "metadata": {},
   "source": [
    "This pretrianed model performs way better than the model you just trained for a few epochs using a single dataset. Still, pretrained models cannot be used for specific tasks, such as sentiment extraction or sequence classification. This is why supervised fine-tuning methods are introduced.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
